---
title: "[GitHub] GitHub Action 사용하여 자동 크롤링과 Push 구현하기"
date: 2020-04-02 11:30:00 +0800
categories: [1.Studying, GitHub]
tags: [GitHub, GitHubAction, crawler, 크롤러, 크롤링]
---



------

# GitHub Action 사용하여 자동 크롤링과 Push 구현하기



GitHub 저장소를 생성한 후에 들어가 보면, 탭 메뉴 중에 **Action**이라는 메뉴가 있습니다.

여기에 Workflow 파일을 생성해 주면, **자동으로 GitHub가 지정된 행동을 실행**해 줍니다.

이것을 응용하면, GitHub page 를 만들어 배포하였을 때, 내부의 데이터를 **일정 시간 간격마다 자동으로 크롤러가 실행되어 push까지 되게끔** 설계할 수 있습니다.



그럼 먼저 자동으로 실행할 크롤러를 파이썬으로 간단하게 만들어 보겠습니다.

------



## **1. 파이썬으로 크롤러 만들기**

먼저 크롤링할 사이트를 고릅니다. 저는 연합뉴스 사이트의 코로나19 관련 뉴스 페이지에 들어갔습니다.

> 링크 : https://www.yna.co.kr/safe/news

![](https://i.imgur.com/aDxDqfr.png)

여기에서 위의 뉴스 제목들을 크롤링해 보겠습니다.

![](https://i.imgur.com/yh0TWwk.png)

> **[그림 1] 개발자 도구로 크롤링할 대상 파악**

웹 브라우저에서 F12(개발자 도구)를 누른 후 해당 요소를 찾아갑니다. 위의 div > h3 구조 안의 a 태그에 뉴스 기사 제목과 URL이 들어있음을 확인할 수 있습니다.

------

![](https://i.imgur.com/jlqZLPs.png)

> **[그림 2] JS path 복사하기**

파이썬 코드에 크롤링할 컨텐츠의 경로를 찾기 위한 과정입니다.

모든 a 태그들을 크롤링해야 하므로, 그 위의 h3 태그에서 **마우스 우클릭 - Copy - Copy JS path**를 선택합니다.

------

![](https://i.imgur.com/ruuChF2.png)

> **[그림 3] JS Path 복사 결과**

그러면 위와 같은 경로를 얻게 됩니다. 이를 참고하여 파이썬 코드를 작성해 보겠습니다.

------

```python
import requests
from bs4 import BeautifulSoup
import json
import os
import sys

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

print('뉴스기사크롤러 시작')

req = requests.get('https://www.yna.co.kr/safe/news')
req.encoding= None
html = req.content
soup = BeautifulSoup(html, 'html.parser')
datas = soup.select(
    'div.contents > div.content01 > div > ul > li >article > div >h3'
    )

data = {}

for title in datas:   
    name = title.find_all('a')[0].text
    url = 'http:'+title.find('a')['href']
    data[name] = url

with open(os.path.join(BASE_DIR, 'news.json'), 'w+',encoding='utf-8') as json_file:
    json.dump(data, json_file, ensure_ascii = False, indent='\t')

print('뉴스기사크롤러 끝')
```

* 크롤링에 필요한 BeautifulSoup 과 얻어진 데이터를 json 파일로 담기 위한 json 등 필요한 모듈들을 import 합니다.
  * 해당 모듈이 설치되어 있지 않는 경우, cmd에서 **pip install 모듈명** 으로 설치합니다.
* BASE_DIR 에는 현재 파이썬 파일이 위치한 곳의 경로를 담습니다.
* soup.select에 구한 경로를 넣습니다.
  * div.contents 는 class명이 contents인 div를 말합니다.
  * '>' 표현은 해당 요소의 바로 한 단계 아래에 있는 요소를 나타냅니다.
  * 크롤링할 h3 태그 전까지 위에 있는 경로들이 div.contents > div.content01 > div > ul > ... > 라는 의미입니다.

* find_all(태그명) : 해당 태그명이 붙은 모든 데이터를 불러옵니다.
* find('a')['href'] : a 태그의 href 에 있는 값을 가져옵니다.
* with 문에서는 **데이터를 json파일에 저장하기 위한 코드를 작성하였습니다.**

이렇게 만든 다음 cmd에서 파이썬 파일을 실행해 봅니다.

------

![](https://i.imgur.com/qSSgtlr.png)

![](https://i.imgur.com/LODtx3Z.png)

> **[그림 4] 크롤링이 정상적으로 완료됨**

위 그림처럼 크롤링이 잘 되었음을 확인할 수 있습니다. 그리고 데이터는 json 파일로 저장되었습니다.

------

## 2. GitHub Action으로 크롤러 자동 실행 및 Push 하기

이렇게 크롤러 파일을 만들어 두면, GitHub Action 에서 이 파일을 주기적으로 자동 실행하고 Push 까지 되도록 Workflow를 작성할 수 있습니다.

